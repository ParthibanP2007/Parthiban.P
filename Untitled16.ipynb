{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-ep2SjZuvesf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import io\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, auc\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.models import load_model\n",
        "from flask import Flask, request, jsonify, send_file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "SEED = 42\n",
        "NUM_EPOCHS = 12\n",
        "LEARNING_RATE = 1e-4\n",
        "MODEL_PATH = \"covid_effnetb0.h5\"\n",
        "CLASS_MODE = \"binary\"  # binary classification: COVID (1) vs Normal (0)\n",
        "DATA_DIR = \"dataset\"  # change if your dataset folder is different\n",
        "\n",
        "# -------------"
      ],
      "metadata": {
        "id": "IfeIDJopvfrl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data generators\n",
        "# -------------------------\n",
        "def get_generators(train_dir=os.path.join(DATA_DIR,\"train\"),\n",
        "                   val_dir=os.path.join(DATA_DIR,\"val\"),\n",
        "                   test_dir=os.path.join(DATA_DIR,\"test\"),\n",
        "                   img_size=IMG_SIZE, batch_size=BATCH_SIZE):\n",
        "    # augmentation for training\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=12,\n",
        "        width_shift_range=0.08,\n",
        "        height_shift_range=0.08,\n",
        "        shear_range=0.08,\n",
        "        zoom_range=0.08,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode=\"nearest\"\n",
        "    )\n",
        "\n",
        "    # for validation and test: only rescale\n",
        "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_gen = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(img_size, img_size),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=CLASS_MODE,\n",
        "        seed=SEED\n",
        "    )\n",
        "\n",
        "    val_gen = val_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(img_size, img_size),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=CLASS_MODE,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    test_gen = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(img_size, img_size),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=CLASS_MODE,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_gen, val_gen, test_gen"
      ],
      "metadata": {
        "id": "fSWQDGSOvftj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xCu6rZA0vfv5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model (transfer learning)\n",
        "# -------------------------\n",
        "def build_model(img_size=IMG_SIZE, lr=LEARNING_RATE, fine_tune_at=None):\n",
        "    base = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(img_size, img_size, 3))\n",
        "    base.trainable = False\n",
        "\n",
        "    x = base.output\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    x = layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = models.Model(inputs=base.input, outputs=outputs)\n",
        "\n",
        "    model.compile(optimizer=optimizers.Adam(lr), loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")])\n",
        "\n",
        "    # Optionally unfreeze a portion for fine-tuning\n",
        "    if fine_tune_at is not None:\n",
        "        base.trainable = True\n",
        "        for layer in base.layers[:fine_tune_at]:\n",
        "            layer.trainable = False\n",
        "        # recompile with a lower LR\n",
        "        model.compile(optimizer=optimizers.Adam(lr/10),\n",
        "                      loss=\"binary_crossentropy\",\n",
        "                      metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "uBy372JbvfyB"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m-eyviGJvf0D"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "def train(save_path=MODEL_PATH, epochs=NUM_EPOCHS):\n",
        "    train_gen, val_gen, _ = get_generators()\n",
        "    model = build_model()\n",
        "\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(save_path, monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
        "        EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Optionally fine-tune: unfreeze last blocks and train for a few more epochs\n",
        "    # Example: unfreeze top 20 layers\n",
        "    print(\"\\n--- Starting fine-tuning ---\\n\")\n",
        "    model = build_model(fine_tune_at=-20)  # unfreeze last 20 layers\n",
        "    # load weights from prior best model if saved\n",
        "    if os.path.exists(save_path):\n",
        "        model.load_weights(save_path)\n",
        "    # re-train for small number of epochs\n",
        "    ft_history = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        epochs=5,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # save final\n",
        "    model.save(save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "    return history"
      ],
      "metadata": {
        "id": "0inP12Mpvf28"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EPEUPQJjvf82"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p83sIwmXvf-y"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training curves\n",
        "# -------------------------\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12,4))\n",
        "\n",
        "    # accuracy\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(history.history.get(\"accuracy\", []), label=\"train_acc\")\n",
        "    plt.plot(history.history.get(\"val_accuracy\", []), label=\"val_acc\")\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    # loss\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(history.history.get(\"loss\", []), label=\"train_loss\")\n",
        "    plt.plot(history.history.get(\"val_loss\", []), label=\"val_loss\")\n",
        "    plt.title(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "m0sACAj5vgCM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation utilities\n",
        "# -------------------------\n",
        "def evaluate_model(model_path=MODEL_PATH):\n",
        "    _, val_gen, test_gen = get_generators()\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Model weights not found at {model_path}\")\n",
        "\n",
        "    model = load_model(model_path, compile=False)\n",
        "    model.compile(optimizer=optimizers.Adam(LEARNING_RATE), loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")])\n",
        "\n",
        "    # Evaluate on validation and test\n",
        "    print(\"Evaluating on validation set:\")\n",
        "    val_results = model.evaluate(val_gen, verbose=1)\n",
        "    print(\"Evaluating on test set:\")\n",
        "    test_results = model.evaluate(test_gen, verbose=1)\n",
        "\n",
        "    # Predictions and metrics (test set)\n",
        "    y_true = test_gen.classes\n",
        "    y_pred_probs = model.predict(test_gen, verbose=1).ravel()\n",
        "    y_pred = (y_pred_probs >= 0.5).astype(int)\n",
        "\n",
        "    print(\"\\nClassification Report (test set):\")\n",
        "    print(classification_report(y_true, y_pred, target_names=list(test_gen.class_indices.keys())))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    plot_confusion_matrix(cm, classes=list(test_gen.class_indices.keys()), normalize=True,\n",
        "                          title=\"Normalized Confusion Matrix (Test)\")\n",
        "\n",
        "    # ROC-AUC\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(y_true, y_pred_probs)\n",
        "        print(f\"Test ROC-AUC: {roc_auc:.4f}\")\n",
        "        plot_roc(y_true, y_pred_probs)\n",
        "    except Exception as e:\n",
        "        print(\"ROC-AUC couldn't be computed:\", e)\n",
        "\n",
        "    return {\"val_results\": val_results, \"test_results\": test_results, \"cm\": cm}\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-9)\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues if cmap is None else cmap)\n",
        "    plt.title(title)\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc(y_true, y_score):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0,1],[0,1], linestyle='--', lw=2)\n",
        "    plt.xlim([0.0,1.0]); plt.ylim([0.0,1.05])\n",
        "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "pQjSYOfkwBkQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fxtU0-8IwBmW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X52Xse5HwBoJ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yH67-1SQwBqP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wWOpRDYxwI_w"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Grad-CAM\n",
        "# -------------------------\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    \"\"\"Returns heatmap for Grad-CAM.\"\"\"\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(predictions[0])\n",
        "        class_channel = predictions[:, 0]  # binary classification (sigmoid)\n",
        "    grads = tape.gradient(class_channel, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "    heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + 1e-9)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "def grad_cam_and_overlay(img_path, model_path=MODEL_PATH, last_conv_layer_name=\"top_conv\"):\n",
        "    \"\"\"\n",
        "    Produces Grad-CAM overlay and displays it.\n",
        "    last_conv_layer_name depends on base model (for EfficientNetB0, 'top_conv' typically exists).\n",
        "    \"\"\"\n",
        "    model = load_model(model_path, compile=False)\n",
        "    # preprocess image\n",
        "    orig = Image.open(img_path).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n",
        "    img = np.array(orig) / 255.0\n",
        "    input_arr = np.expand_dims(img, axis=0)\n",
        "\n",
        "    preds = model.predict(input_arr)\n",
        "    prob = preds[0][0]\n",
        "    print(f\"Predicted probability (COVID=1): {prob:.4f}\")\n",
        "\n",
        "    heatmap = make_gradcam_heatmap(input_arr, model, last_conv_layer_name)\n",
        "    heatmap = cv2.resize(heatmap, (IMG_SIZE, IMG_SIZE))\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    overlay = cv2.addWeighted(np.array(orig), 0.6, heatmap_color, 0.4, 0)\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.subplot(1,2,1); plt.title(\"Original\"); plt.axis('off'); plt.imshow(orig)\n",
        "    plt.subplot(1,2,2); plt.title(\"Grad-CAM overlay\"); plt.axis('off'); plt.imshow(overlay)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "1P8km6IFwJB6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Simple Flask app for inference\n",
        "# -------------------------\n",
        "def create_app(model_path=MODEL_PATH):\n",
        "    app = Flask(__name__)\n",
        "    model = None\n",
        "\n",
        "    @app.before_first_request\n",
        "    def load():\n",
        "        nonlocal model\n",
        "        if not os.path.exists(model_path):\n",
        "            raise RuntimeError(f\"Model file not found at {model_path}. Train the model first.\")\n",
        "        model = load_model(model_path, compile=False)\n",
        "        print(\"Model loaded for inference.\")\n",
        "\n",
        "    @app.route(\"/predict\", methods=[\"POST\"])\n",
        "    def predict_route():\n",
        "        \"\"\"\n",
        "        Accepts multipart/form-data with 'file' = image\n",
        "        Returns JSON: {'pred_prob': float, 'pred_class': 'COVID' or 'Normal'}\n",
        "        \"\"\"\n",
        "        if 'file' not in request.files:\n",
        "            return jsonify({\"error\": \"no file part\"}), 400\n",
        "        file = request.files['file']\n",
        "        # ... rest of the prediction logic"
      ],
      "metadata": {
        "id": "_2I4BSrOwJEH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MbE2xn2hwJGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "960g9Y0vwJIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jsp81nb9wJL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AcQ1tc1ZwBtc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}